{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/pw.py append\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PW Miniproject\n",
    "## Introduction\n",
    "\n",
    "The objective of this miniproject is to exercise your ability to use basic Python data structures, define functions, and control program flow. We will be using these concepts to perform some fundamental data wrangling tasks such as joining data sets together, splitting data into groups, and aggregating data into summary statistics.\n",
    "**Please do not use `pandas` or `numpy` to answer these questions.**\n",
    "\n",
    "We will be working with medical data from the British NHS on prescription drugs. Since this is real data, it contains many ambiguities that we will need to confront in our analysis. This is commonplace in data science, and is one of the lessons you will learn in this miniproject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "We first need to download the data we'll be using from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-05-30 08:43:58--  http://dataincubator-wqu.s3.amazonaws.com/pwdata/201701scripts_sample.json.gz\n",
      "Resolving dataincubator-wqu.s3.amazonaws.com (dataincubator-wqu.s3.amazonaws.com)... 52.216.113.187\n",
      "Connecting to dataincubator-wqu.s3.amazonaws.com (dataincubator-wqu.s3.amazonaws.com)|52.216.113.187|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10367709 (9.9M) [application/json]\n",
      "Saving to: ‘./pw-data/201701scripts_sample.json.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 3.72M 3s\n",
      "    50K .......... .......... .......... .......... ..........  0% 7.43M 2s\n",
      "   100K .......... .......... .......... .......... ..........  1% 6.93M 2s\n",
      "   150K .......... .......... .......... .......... ..........  1% 69.3M 1s\n",
      "   200K .......... .......... .......... .......... ..........  2% 8.44M 1s\n",
      "   250K .......... .......... .......... .......... ..........  2% 40.2M 1s\n",
      "   300K .......... .......... .......... .......... ..........  3% 49.5M 1s\n",
      "   350K .......... .......... .......... .......... ..........  3% 86.2M 1s\n",
      "   400K .......... .......... .......... .......... ..........  4% 88.0M 1s\n",
      "   450K .......... .......... .......... .......... ..........  4% 12.9M 1s\n",
      "   500K .......... .......... .......... .......... ..........  5% 34.8M 1s\n",
      "   550K .......... .......... .......... .......... ..........  5% 73.7M 1s\n",
      "   600K .......... .......... .......... .......... ..........  6% 82.2M 1s\n",
      "   650K .......... .......... .......... .......... ..........  6%  351M 1s\n",
      "   700K .......... .......... .......... .......... ..........  7%  351M 1s\n",
      "   750K .......... .......... .......... .......... ..........  7%  355M 1s\n",
      "   800K .......... .......... .......... .......... ..........  8%  412M 0s\n",
      "   850K .......... .......... .......... .......... ..........  8%  412M 0s\n",
      "   900K .......... .......... .......... .......... ..........  9%  417M 0s\n",
      "   950K .......... .......... .......... .......... ..........  9%  365M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 10% 13.4M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 10% 23.6M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 11% 76.2M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 11% 89.6M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 12% 76.7M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 12% 74.5M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 13% 80.3M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 13%  189M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 14%  406M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 14%  346M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 15%  205M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 15%  400M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 16% 65.4M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 16% 38.0M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 17% 35.4M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 17% 21.0M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 18% 80.3M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 18% 46.4M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 19% 84.5M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 19% 85.1M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 20% 32.5M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 20% 75.5M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 21%  332M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 21%  354M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 22%  394M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 22%  356M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 23%  401M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 23%  402M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 24%  383M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 24%  243M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 25% 72.4M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 25% 88.2M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 26% 10.5M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 26% 48.4M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 27% 14.5M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 27% 61.5M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 28% 62.3M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 28% 15.1M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 29% 72.2M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 29% 85.8M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 30% 22.4M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 30% 22.6M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 31% 87.1M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 31%  114M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 32% 27.7M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 32% 69.5M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 33%  124M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 33%  224M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 34%  357M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 34%  261M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 35%  401M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 35%  403M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 36%  305M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 36%  318M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 37% 49.2M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 37% 65.9M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 38% 82.6M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 38% 78.4M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 39% 87.4M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 39% 81.1M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 40% 62.4M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 40% 52.5M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 40% 82.8M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 41% 50.8M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 41%  312M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 42%  277M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 42%  398M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 43%  361M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 43%  289M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 44% 57.5M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 44% 81.4M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 45% 82.1M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 45% 50.2M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 46% 53.8M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 46% 82.0M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 47%  368M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 47%  275M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 48%  203M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 48%  395M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 49%  381M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 49%  231M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 50%  225M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 50%  380M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 51%  387M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 51%  399M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 52%  297M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 52%  283M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 53%  393M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 53%  188M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 54%  311M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 54%  367M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 55%  409M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 55%  382M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 56%  327M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 56%  403M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 57%  351M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 57%  347M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 58%  113M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 58% 70.2M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 59% 69.1M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 59% 67.0M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 60% 62.0M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 60% 85.6M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 61% 77.7M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 61% 88.1M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 62% 75.5M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 62% 77.9M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 63% 89.1M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 63% 85.4M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 64% 66.4M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 64% 74.9M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 65% 86.4M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 65% 89.7M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 66% 95.8M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 66%  249M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 67%  388M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 67%  406M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 68%  320M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 68%  333M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 69%  407M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 69%  401M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 70%  342M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 70%  416M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 71%  370M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 71%  378M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 72%  280M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 72%  406M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 73%  404M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 73%  364M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 74%  353M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 74%  400M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 75%  408M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 75%  408M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 76%  313M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 76%  294M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 77%  395M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 77%  373M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 78%  332M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 78%  368M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 79%  411M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 79%  410M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 80%  280M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 80%  399M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 80% 7.13M 0s\n",
      "  8200K .......... .......... .......... .......... .......... 81% 53.8M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 81% 24.0M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 82% 15.8M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 82% 62.0M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 83%  260M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 83%  267M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 84%  276M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 84%  226M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 85%  269M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 85%  249M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 86%  278M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 86%  240M 0s\n",
      "  8800K .......... .......... .......... .......... .......... 87%  286M 0s\n",
      "  8850K .......... .......... .......... .......... .......... 87%  273M 0s\n",
      "  8900K .......... .......... .......... .......... .......... 88%  286M 0s\n",
      "  8950K .......... .......... .......... .......... .......... 88%  230M 0s\n",
      "  9000K .......... .......... .......... .......... .......... 89%  282M 0s\n",
      "  9050K .......... .......... .......... .......... .......... 89%  276M 0s\n",
      "  9100K .......... .......... .......... .......... .......... 90%  279M 0s\n",
      "  9150K .......... .......... .......... .......... .......... 90%  239M 0s\n",
      "  9200K .......... .......... .......... .......... .......... 91%  253M 0s\n",
      "  9250K .......... .......... .......... .......... .......... 91%  262M 0s\n",
      "  9300K .......... .......... .......... .......... .......... 92%  282M 0s\n",
      "  9350K .......... .......... .......... .......... .......... 92%  236M 0s\n",
      "  9400K .......... .......... .......... .......... .......... 93%  273M 0s\n",
      "  9450K .......... .......... .......... .......... .......... 93% 57.1M 0s\n",
      "  9500K .......... .......... .......... .......... .......... 94% 54.3M 0s\n",
      "  9550K .......... .......... .......... .......... .......... 94% 44.9M 0s\n",
      "  9600K .......... .......... .......... .......... .......... 95% 53.2M 0s\n",
      "  9650K .......... .......... .......... .......... .......... 95% 52.4M 0s\n",
      "  9700K .......... .......... .......... .......... .......... 96% 55.0M 0s\n",
      "  9750K .......... .......... .......... .......... .......... 96% 45.0M 0s\n",
      "  9800K .......... .......... .......... .......... .......... 97% 54.8M 0s\n",
      "  9850K .......... .......... .......... .......... .......... 97% 59.4M 0s\n",
      "  9900K .......... .......... .......... .......... .......... 98%  265M 0s\n",
      "  9950K .......... .......... .......... .......... .......... 98%  240M 0s\n",
      " 10000K .......... .......... .......... .......... .......... 99%  273M 0s\n",
      " 10050K .......... .......... .......... .......... .......... 99%  258M 0s\n",
      " 10100K .......... .......... ....                            100%  256M=0.1s\n",
      "\n",
      "2021-05-30 08:43:58 (66.7 MB/s) - ‘./pw-data/201701scripts_sample.json.gz’ saved [10367709/10367709]\n",
      "\n",
      "--2021-05-30 08:43:58--  http://dataincubator-wqu.s3.amazonaws.com/pwdata/practices.json.gz\n",
      "Resolving dataincubator-wqu.s3.amazonaws.com (dataincubator-wqu.s3.amazonaws.com)... 52.216.113.187\n",
      "Connecting to dataincubator-wqu.s3.amazonaws.com (dataincubator-wqu.s3.amazonaws.com)|52.216.113.187|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 402461 (393K) [application/json]\n",
      "Saving to: ‘./pw-data/practices.json.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 12% 3.80M 0s\n",
      "    50K .......... .......... .......... .......... .......... 25% 7.15M 0s\n",
      "   100K .......... .......... .......... .......... .......... 38% 7.96M 0s\n",
      "   150K .......... .......... .......... .......... .......... 50% 58.8M 0s\n",
      "   200K .......... .......... .......... .......... .......... 63% 79.3M 0s\n",
      "   250K .......... .......... .......... .......... .......... 76% 77.2M 0s\n",
      "   300K .......... .......... .......... .......... .......... 89% 9.87M 0s\n",
      "   350K .......... .......... .......... .......... ...       100% 72.7M=0.03s\n",
      "\n",
      "2021-05-30 08:43:58 (11.5 MB/s) - ‘./pw-data/practices.json.gz’ saved [402461/402461]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir pw-data\n",
    "wget http://dataincubator-wqu.s3.amazonaws.com/pwdata/201701scripts_sample.json.gz -nc -P ./pw-data\n",
    "wget http://dataincubator-wqu.s3.amazonaws.com/pwdata/practices.json.gz -nc -P ./pw-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The first step of the project is to read in the data. We will discuss reading and writing various kinds of files later in the course, but the code below should get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./pw-data/201701scripts_sample.json.gz', 'rb') as f:\n",
    "    scripts = json.load(f)\n",
    "\n",
    "with gzip.open('./pw-data/practices.json.gz', 'rb') as f:\n",
    "    practices = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set comes from Britain's National Health Service. The `scripts` variable is a list of prescriptions issued by NHS doctors. Each prescription is represented by a dictionary with various data fields: `'practice'`, `'bnf_code'`, `'bnf_name'`, `'quantity'`, `'items'`, `'nic'`, and `'act_cost'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bnf_code': '0101010G0AAABAB',\n",
       "  'items': 2,\n",
       "  'practice': 'N81013',\n",
       "  'bnf_name': 'Co-Magaldrox_Susp 195mg/220mg/5ml S/F',\n",
       "  'nic': 5.98,\n",
       "  'act_cost': 5.56,\n",
       "  'quantity': 1000},\n",
       " {'bnf_code': '0101021B0AAAHAH',\n",
       "  'items': 1,\n",
       "  'practice': 'N81013',\n",
       "  'bnf_name': 'Alginate_Raft-Forming Oral Susp S/F',\n",
       "  'nic': 1.95,\n",
       "  'act_cost': 1.82,\n",
       "  'quantity': 500}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [glossary of terms](http://webarchive.nationalarchives.gov.uk/20180328130852tf_/http://content.digital.nhs.uk/media/10686/Download-glossary-of-terms-for-GP-prescribing---presentation-level/pdf/PLP_Presentation_Level_Glossary_April_2015.pdf/) and [FAQ](http://webarchive.nationalarchives.gov.uk/20180328130852tf_/http://content.digital.nhs.uk/media/10048/FAQs-Practice-Level-Prescribingpdf/pdf/PLP_FAQs_April_2015.pdf/) is available from the NHS regarding the data. Below we supply a data dictionary briefly describing what these fields mean.\n",
    "\n",
    "| Data field |Description|\n",
    "|:----------:|-----------|\n",
    "|`'practice'`|Code designating the medical practice issuing the prescription|\n",
    "|`'bnf_code'`|British National Formulary drug code|\n",
    "|`'bnf_name'`|British National Formulary drug name|\n",
    "|`'quantity'`|Number of capsules/quantity of liquid/grams of powder prescribed|\n",
    "| `'items'`  |Number of refills (e.g. if `'quantity'` is 30 capsules, 3 `'items'` means 3 bottles of 30 capsules)|\n",
    "|  `'nic'`   |Net ingredient cost|\n",
    "|`'act_cost'`|Total cost including containers, fees, and discounts|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `practices` variable is a list of member medical practices of the NHS. Each practice is represented by a dictionary containing identifying information for the medical practice. Most of the data fields are self-explanatory. Notice the values in the `'code'` field of `practices` match the values in the `'practice'` field of `scripts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 'A81001',\n",
       "  'name': 'THE DENSHAM SURGERY',\n",
       "  'addr_1': 'THE HEALTH CENTRE',\n",
       "  'addr_2': 'LAWSON STREET',\n",
       "  'borough': 'STOCKTON ON TEES',\n",
       "  'village': 'CLEVELAND',\n",
       "  'post_code': 'TS18 1HU'},\n",
       " {'code': 'A81002',\n",
       "  'name': 'QUEENS PARK MEDICAL CENTRE',\n",
       "  'addr_1': 'QUEENS PARK MEDICAL CTR',\n",
       "  'addr_2': 'FARRER STREET',\n",
       "  'borough': 'STOCKTON ON TEES',\n",
       "  'village': 'CLEVELAND',\n",
       "  'post_code': 'TS18 2AW'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practices[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following questions we will ask you to explore this data set. You may need to combine pieces of the data set together in order to answer some questions. Not every element of the data set will be used in answering the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: summary_statistics\n",
    "\n",
    "Our beneficiary data (`scripts`) contains quantitative data on the number of items dispensed (`'items'`), the total quantity of item dispensed (`'quantity'`), the net cost of the ingredients (`'nic'`), and the actual cost to the patient (`'act_cost'`). Whenever working with a new data set, it can be useful to calculate summary statistics to develop a feeling for the volume and character of the data. This makes it easier to spot trends and significant features during further stages of analysis.\n",
    "\n",
    "Calculate the sum, mean, standard deviation, and quartile statistics for each of these quantities. Format your results for each quantity as a list: `[sum, mean, standard deviation, 1st quartile, median, 3rd quartile]`. We'll create a `tuple` with these lists for each quantity as a final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "def quantile(values, q):\n",
    "    idx = len(values) * q\n",
    "    if idx == int(idx):\n",
    "        return sorted(values)[idx]\n",
    "    else:\n",
    "        f1 = floor(idx)\n",
    "        c1 = ceil(idx)\n",
    "        return sum(sorted(values)[f1:c1+1]) / 2\n",
    "    \n",
    "my_list = [0, 1, 3, 4, 50, 6, 7, 80, 9, 100, 11, 12, 13]\n",
    "\n",
    "quantile(my_list,0.25)\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "def mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def std(values, avg):\n",
    "    numerator = sum([(x-avg)**2 for x in values])\n",
    "    denominator = len(values)\n",
    "    return (numerator/denominator)**0.5\n",
    "\n",
    "def quantile(values, q):\n",
    "    idx = int(ceil(len(values) * q))\n",
    "    return sorted(values)[idx]\n",
    "    \n",
    "def describe(key):\n",
    "    values = [d[key] for d in scripts]\n",
    "    total  = sum(values)\n",
    "    avg    = mean(values)\n",
    "    s      = std(values, avg)\n",
    "    q25 = quantile(values, 0.25)\n",
    "    med = quantile(values, 0.5)\n",
    "    q75 =  quantile(values, 0.75)\n",
    "\n",
    "    return (total, avg, s, q25, med, q75)\n",
    "summary = [('items', describe('items')),\n",
    "           ('quantity', describe('quantity')),\n",
    "           ('nic', describe('nic')),\n",
    "           ('act_cost', describe('act_cost'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.pw__summary_statistics(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: most_common_item\n",
    "\n",
    "Often we are not interested only in how the data is distributed in our entire data set, but within particular groups -- for example, how many items of each drug (i.e. `'bnf_name'`) were prescribed? Calculate the total items prescribed for each `'bnf_name'`. What is the most commonly prescribed `'bnf_name'` in our data?\n",
    "\n",
    "To calculate this, we first need to split our data set into groups corresponding with the different values of `'bnf_name'`. Then we can sum the number of items dispensed within in each group. Finally we can find the largest sum.\n",
    "\n",
    "We'll use `'bnf_name'` to construct our groups. You should have *5619* unique values for `'bnf_name'`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnf_names = {d['bnf_name'] for d in scripts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5619"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bnf_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnf_names = {d['bnf_name'] for d in scripts}\n",
    "assert(len(bnf_names) == 5619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to construct \"groups\" identified by `'bnf_name'`, where each group is a collection of prescriptions (i.e. dictionaries from `scripts`). We'll construct a dictionary called `groups`, using `bnf_names` as the keys. We'll represent a group with a `list`, since we can easily append new members to the group. To split our `scripts` into groups by `'bnf_name'`, we should iterate over `scripts`, appending prescription dictionaries to each group as we encounter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {name: [] for name in bnf_names}\n",
    "for script in scripts:\n",
    "    groups[script['bnf_name']].append(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mefix 10cm x 5m Surg Adh Tape Perm Non-W',\n",
       " 'Co-Tenidone_Tab 50mg/12.5mg',\n",
       " 'Ropinirole HCl_Tab 250mcg']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(groups.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [(name, sum([d['items'] for d in group]))\n",
    "         for name, group in groups.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've constructed our groups we should sum up `'items'` in each group and find the `'bnf_name'` with the largest sum. The result, `max_item`, should have the form `[(bnf_name, item total)]`, e.g. `[('Foobar', 2000)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item = [max(items, key=lambda tup: tup[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP:** If you are getting an error from the grader below, please make sure your answer conforms to the correct format of `[(bnf_name, item total)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.pw__most_common_item(max_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Write a function that constructs groups as we did above. The function should accept a list of dictionaries (e.g. `scripts` or `practices`) and a tuple of fields to `groupby` (e.g. `('bnf_name',)` or `('bnf_name', 'post_code')`) and returns a dictionary of groups. The following questions will require you to aggregate data in groups, so this could be a useful function for the rest of the miniproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique names\n",
    "bnf_names = {d['bnf_name'] for d in scripts}\n",
    "\n",
    "#Use unique names to create group dict\n",
    "groups = {name: [] for name in bnf_names}\n",
    "\n",
    "#Fill group dict from items in JSON(scripts)\n",
    "for script in scripts:\n",
    "    groups[script['bnf_name']].append(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_by_field(data, fields):\n",
    "    #Get unique names\n",
    "    names = {tuple(d[field] for field in fields) for d in data}\n",
    "\n",
    "    #Use unique names to create group dict\n",
    "    groups = {name: [] for name in names}\n",
    "\n",
    "    #Fill group dict from items in JSON(scripts)\n",
    "    for item in data:\n",
    "        key = tuple(item[field] for field in fields)\n",
    "        groups[key].append(item)\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mefix 10cm x 5m Surg Adh Tape Perm Non-W', [{'bnf_code': '20100000572', 'items': 1, 'practice': 'N81013', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'N81062', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.4, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'N81085', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'Y03882', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'N81016', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.38, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'N81053', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.38, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 7, 'practice': 'Y03881', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 49.3, 'act_cost': 45.64, 'quantity': 17}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'N81024', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.76, 'quantity': 4}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'N81040', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.4, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'Y03880', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'P85003', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'P85608', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.38, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'A89603', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'A89604', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'B81031', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.08, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 6, 'practice': 'B81077', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 17.4, 'act_cost': 16.17, 'quantity': 6}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'B81091', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.38, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'C84013', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.36, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'C84035', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.68, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'M86005', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.07, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'Y00996', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'M88619', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.08, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'M88625', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'M88646', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'C82047', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.07, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'C82075', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.37, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 4, 'practice': 'M83075', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.79, 'quantity': 4}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'M83623', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.08, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 3, 'practice': 'F82014', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 8.7, 'act_cost': 8.09, 'quantity': 3}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'F82016', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.75, 'quantity': 4}, {'bnf_code': '20100000572', 'items': 3, 'practice': 'E84003', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.77, 'quantity': 4}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'E84626', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 2, 'practice': 'E85058', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.39, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'E85126', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'E85605', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.75, 'quantity': 4}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'E85732', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.68, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'L84008', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'L84049', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'L83008', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 5.8, 'act_cost': 5.38, 'quantity': 2}, {'bnf_code': '20100000572', 'items': 5, 'practice': 'L83100', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 17.4, 'act_cost': 16.16, 'quantity': 6}, {'bnf_code': '20100000572', 'items': 1, 'practice': 'J81041', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 2.9, 'act_cost': 2.7, 'quantity': 1}, {'bnf_code': '20100000572', 'items': 3, 'practice': 'J81046', 'bnf_name': 'Mefix 10cm x 5m Surg Adh Tape Perm Non-W', 'nic': 11.6, 'act_cost': 10.76, 'quantity': 4}])\n"
     ]
    }
   ],
   "source": [
    "print(list(groups.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = group_by_field(scripts, ('bnf_name',))\n",
    "\n",
    "items = [(name, sum([d['items'] for d in group]))\n",
    "         for name, group in groups.items()]\n",
    "\n",
    "test_max_item = max(items, key=lambda tup: tup[1])\n",
    "\n",
    "test_max_item = [(test_max_item[0][0], test_max_item[1])]\n",
    "\n",
    "assert test_max_item == max_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: postal_totals\n",
    "\n",
    "Our data set is broken up among different files. This is typical for tabular data to reduce redundancy. Each table typically contains data about a particular type of event, processes, or physical object. Data on prescriptions and medical practices are in separate files in our case. If we want to find the total items prescribed in each postal code, we will have to _join_ our prescription data (`scripts`) to our clinic data (`practices`).\n",
    "\n",
    "Find the total items prescribed in each postal code, representing the results as a list of tuples `(post code, total items prescribed)`. Sort your results ascending alphabetically by post code and take only results from the first 100 post codes. Only include post codes if there is at least one prescription from a practice in that post code.\n",
    "\n",
    "**NOTE:** Some practices have multiple postal codes associated with them. Use the alphabetically first postal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join `scripts` and `practices` based on the fact that `'practice'` in `scripts` matches `'code'` in `practices`. However, we must first deal with the repeated values of `'code'` in `practices`. We want the alphabetically first postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_postal = {}\n",
    "for practice in practices:\n",
    "    if practice['code'] in practice_postal:\n",
    "        practice_postal[practice['code']] = practice['post_code'] if practice['post_code'] < practice_postal[practice['code']] else practice_postal[practice['code']]\n",
    "    else:\n",
    "        practice_postal[practice['code']] = practice['post_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** This is an aggregation of the practice data grouped by practice codes. Write an alternative implementation of the above cell using the `group_by_field` function you defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert practice_postal['K82019'] == 'HP21 8TR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join `practice_postal` to `scripts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = scripts[:]\n",
    "for script in joined:\n",
    "    script['post_code'] = practice_postal[script['practice']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll group the prescription dictionaries in `joined` by `'post_code'` and sum up the items prescribed in each group, as we did in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_by_post = {}\n",
    "for join in joined:\n",
    "    if join['post_code'] in items_by_post:\n",
    "        items_by_post[join['post_code']]=items_by_post[join['post_code']]+join['items']\n",
    "    else:\n",
    "        items_by_post[join['post_code']]=join['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sort_ = sorted(items_by_post)\n",
    "postal_total = []\n",
    "for key in sort_:\n",
    "    postal_total.append((key, items_by_post[key]))\n",
    "\n",
    "postal_totals = postal_total[:100]\n",
    "\n",
    "grader.score.pw__postal_totals(postal_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: items_by_region\n",
    "\n",
    "Now we'll combine the techniques we've developed to answer a more complex question. Find the most commonly dispensed item in each postal code, representing the results as a list of tuples (`post_code`, `bnf_name`, amount dispensed as proportion of total). Sort your results ascending alphabetically by post code and take only results from the first 100 post codes.\n",
    "\n",
    "**NOTE:** We'll continue to use the `joined` variable we created before, where we've chosen the alphabetically first postal code for each practice. Additionally, some postal codes will have multiple `'bnf_name'` with the same number of items prescribed for the maximum. In this case, we'll take the alphabetically first `'bnf_name'`.\n",
    "\n",
    "There are several approaches to solve this problem but we will guide you through one of them. Feel free to solve it your own way if it is easier for you to understand and implement. If your kernel keeps on dying, it's probably an indication that you are running out of memory. Consider deleting objects you don't need anymore using the `del` statement and shutdown any other running notebooks. For example:\n",
    "```Python\n",
    "del some_object_not_needed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to calculate the total items for each `'post_code'` and `'bnf_name'`. Let's call that result `total_items_by_post_bnf`. Consider what is the best data structure(s) to represent `total_items_by_post_bnf`. It should have 141196 `('post_code', 'bnf_name')` groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_by_field(data,fields):\n",
    "    names={tuple(dict_[field] for field in fields)\n",
    "           for dict_ in data}\n",
    "    groups= {name: [] for name in names}\n",
    "    for dict_ in data:\n",
    "        name = tuple(dict_[field] for field in fields)\n",
    "        groups[name].append(dict_)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_items_by_bnf_post =  {}\n",
    "for key, group in list(group_by_field(joined, ('post_code', 'bnf_name')).items()):\n",
    "    items_total=sum(d['items'] for d in group)\n",
    "    total_items_by_bnf_post[key]=items_total\n",
    "assert len(total_items_by_bnf_post) == 141196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_items = []\n",
    "for (post_code, bnf_name), total in list(total_items_by_bnf_post.items()):\n",
    "        new_dict = {'post_code' : post_code,\n",
    "                    'bnf_name' : bnf_name,\n",
    "                    'total' : total}\n",
    "        total_items.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_by_post = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take `total_items_by_post_bnf` and group it by `'post_code'`. In other words, we want a  data structure that maps a `'post_code'` to a list of all records that belong to that `'post_code'`. There should be 118 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_items_by_post=group_by_field(total_items, ('post_code',))\n",
    "assert len(total_items_by_post) == 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_by_post = max(total_items_by_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with `grouped_post_code`, let's iterate over each group and calculate the following fields for each `'post_code'`:\n",
    "1. the sum of total items for all `'bnf_name'`\n",
    "1. the most total items\n",
    "1. the `'bnf_name'` that had the most total items\n",
    "\n",
    "Once again, consider the best data structure(s) to use to represent the result. It may help to write and use a function when developing your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "get_total = itemgetter('total')\n",
    "max_item_by_post = []\n",
    "groups = list(total_items_by_post.values())\n",
    "for group in groups:\n",
    "    max_total = sorted(group, key=itemgetter('total'), reverse=True)[0]\n",
    "    max_item_by_post.append(max_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_by_post = [sorted(group, key=itemgetter('total'), reverse=True)[0]\n",
    "                    for group in list(total_items_by_post.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_by_region=[]\n",
    "for item in max_item_by_post:\n",
    "    numerator= item['total']\n",
    "    denominator=dict(items_by_post)[item['post_code']]\n",
    "    proportion=numerator/denominator\n",
    "    result=(item['post_code'], item['bnf_name'], proportion)\n",
    "    items_by_region.append(result)\n",
    "items_by_region = sorted(items_by_region)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to:\n",
    "1. calculate the ratio (the amount dispensed as proportion of total)\n",
    "1. [sort](https://docs.python.org/3/howto/sorting.html) alphabetically by the post code\n",
    "1. format the answer as a list of tuples\n",
    "1. take only the first 100 tuples\n",
    "1. submit to the grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score.pw__items_by_region(items_by_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2021 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
